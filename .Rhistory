qplot(temp$TotalArea,temp$SalePrice, data = temp, colour= temp$TotalBath)
#Larger homes would also have better quality fireplaces.
qplot(temp$TotalArea,temp$SalePrice, data = temp, colour= temp$FireplaceQu)
#And better exterior quality
qplot(temp$TotalArea,temp$SalePrice, data = temp, colour= temp$ExterQual)
#And better Basement quality
qplot(temp$TotalArea,temp$SalePrice, data = temp, colour= temp$BsmtQual)
#QUality in general makes for a better house price
qplot(temp$ExterQual,temp$SalePrice, data = temp, colour= temp$BsmtQual)
# Very imporant comparisons, used to make decisions. Great to find patterns
ggplot(data = temp, aes(x=temp$ExterQual, y=temp$SalePrice)) + geom_count(aes( factor(temp$ExterQual) ))
ggplot(data = temp, aes(x=temp$KitchenQual, y=temp$SalePrice)) + geom_count(aes(factor(temp$KitchenQual) ))
ggplot(data = temp, aes(x=temp$BsmtQual, y=temp$SalePrice)) + geom_count(aes(factor(temp$BsmtQual) ))
ggplot(data = temp, aes(x=temp$FireplaceQu, y=temp$SalePrice)) + geom_count(aes(factor(temp$FireplaceQu) ))
#ggplot(data = temp, aes(x=temp$Fireplace, y=temp$SalePrice)) + geom_count(aes((temp$Fireplace) ))
qplot(temp$Fireplaces,temp$SalePrice, data = temp)
ggplot(data = temp, aes(x=temp$MiscVal, y=temp$SalePrice)) + geom_count(aes((temp$MiscVal) ))
ggplot(data = temp, aes(x=temp$BsmtFinSF1, y=temp$SalePrice)) + geom_count(aes((temp$BsmtFinSF1) ))
ggplot(data = temp, aes(x=temp$BsmtFinSF2, y=temp$SalePrice)) + geom_count(aes((temp$BsmtFinSF2) ))
ggplot(data = temp, aes(x=temp$BsmtFinType2, y=temp$SalePrice)) + geom_count(aes((temp$BsmtFinType2) )) #This got through the cracks, it should have been removed.
ggplot(data = temp, aes(x=temp$MasVnrArea, y=temp$SalePrice)) + geom_count(aes((temp$MasVnrArea) ))
ggplot(data = temp, aes(x=temp$MasVnrType, y=temp$SalePrice)) + geom_count(aes((temp$MasVnrType) ))
ggplot(data = temp, aes(x=temp$BsmtUnfSF, y=temp$SalePrice)) + geom_count(aes((temp$BsmtUnfSF) ))
mean(temp$BsmtUnfSF)
nrow(temp[temp$BsmtUnfSF<200,])#I decide to keep this, attribute
ggplot(data = temp, aes(x=temp$LowQualFinSF, y=temp$SalePrice)) + geom_count(aes((temp$LowQualFinSF) ))
ggplot(data = temp, aes(x=temp$GarageCars, y=temp$SalePrice)) + geom_count(aes((temp$GarageCars) ))
ggplot(data = temp, aes(x=temp$LotFrontage, y=temp$SalePrice)) + geom_count(aes((temp$LotFrontage) ))
nrow(temp[temp$LotFrontage<10,])#I decide to keep this, attribute
ggplot(data = temp, aes(x=temp$tot, y=temp$SalePrice)) + geom_count(aes((temp$LotFrontage) ))
mean(temp$PoolArea)
mean(temp$MiscVal)
mean(as.numeric(temp$Fireplaces))
#After observing the graphics and looking into variances and mean, these need to be removed. The means alone tell a story of how skewed of a picture these give. Because of a lack of normalization, means can show a picture as well.
temp$Fireplace <- NULL
temp$PoolArea <- NULL
temp$MiscVal <- NULL
temp$BsmtFinSF2 <- NULL
temp$MasVnrArea <- NULL
temp$LowQualFinSF <-NULL
#bsmtfinsf1
flattened_outlier = unlist(outlier_bsmtFinSF1[1], use.names = FALSE)
temp = subset(temp, !(temp$Id %in% flattened_outlier))
#removed bsmtfinsf2
outlier_bsmtFinSF2 = 0
#removed masvnrarea
outlier_masVnrArea = 0
#bsmtUnfSF
flattened_outlier = unlist(outlier_bsmtUnfSF[1], use.names = FALSE)
temp = subset(temp, !(temp$Id %in% flattened_outlier))
#removed garage area
outlier_garageArea = 0
#removed garage car
flattened_outlier = unlist(outlier_garagecars[1], use.names = FALSE)
temp = subset(temp, !(temp$Id %in% flattened_outlier))
#removed grlivarea
outlier_grLivArea = 0
#lotarea
flattened_outlier = unlist(outlier_lotArea[1], use.names = FALSE)
temp = subset(temp, !(temp$Id %in% flattened_outlier))
#lotfrontage
flattened_outlier = unlist(outlier_lotFrontage[1], use.names = FALSE)
temp = subset(temp, !(temp$Id %in% flattened_outlier))
#removed lowqualfin
outlier_lowQualFinSF = 0
#removed misc
outlier_miscVal = 0
#removed OpenPorch
outlier_openPorch = 0
#removed pool
outlier_poolarea = 0
#removed screen porch
outlier_screenPorch= 0
#removed total Basement SF
outlier_totalBsmtSF= 0
#removed wood deck
outlier_woodDeck= 0
#removed 1st flr sf
outlier_x1stFlrSF= 0
#removed 2nd flr sf
outlier_x2stFlrSF= 0
#removed 3Ss
outlier_X3Ss= 0
print("it is a good idea we waited to remove the outliers, otherwise we would have lost a lot of observations only to remove the attributes later")
#I will allow this outliers:
#outlier_yearbuilt
#after all our intial analysis, we can set the data back to our original name
modified_data = temp
modified_data = subset(modified_data, select=-c(SalePrice))
modified_data$SalePrice = temp$SalePrice
# Our Normalizing technique
normalize <- function(x) {
if (is.numeric(x)){
return ((x - min(x)) / (max(x) - min(x))) }
else{
return (x)
}
}
data_norm = as.data.frame(lapply(modified_data[2:40], normalize))
data_norm <- cbind(modified_data$SalePrice, data_norm)
numsonly <- unlist(lapply(modified_data, is.numeric))
numarray = temp[,numsonly]
numarray = subset(numarray, select = -c(Id))
fit = kmeans(numarray,4)
plotcluster(numarray,fit$cluster)
#str(fit)
fit = kmodes(numarray, 4)
plotcluster(numarray,fit$cluster)
# It is not meaningful to have clustering for categorical variables. I did only numerical values.
set.seed(1)
#Splitting training to 80%, test to 20%
index <- sample(1:nrow(data_norm), 0.80 *nrow(data_norm))
data_train <- data_norm[index,]
data_test <- data_norm[-index,]
#Our label is the Sales price, in col 1
trainlabel <- data_train[,1]
testlabel <- data_test[,1]
#Applying KNN
##test_pred <- knn(train = data_train[,2:39], test = data_test[,2:39],cl = data_train[,1], k=9)
#Creating accuracy matrix
##CrossTable(x=testlabel, y=test_pred, prop.chisq=FALSE)
# install.packages("cowplot")
# library("cowplot")
# install.packages("randomForest")
# library("randomForest")
set.seed(42)
data.imputed = rfImpute(data_train$'modified_data$SalePrice' ~ ., data = data_train, iter = 5)
data.imputed = randomForest(data_train$'modified_data$SalePrice' ~ ., data = data_train, iter = 5)
View(data_train)
# install.packages("cowplot")
# library("cowplot")
# install.packages("randomForest")
# library("randomForest")
set.seed(42)
data.imputed = randomForest(data_train$modified_data$SalePrice ~ ., data = data_train, iter = 5)
colnames(data_norm)[colnames(data_norm=="modified_data$SalePrice")] <- "SalePrice"
colnames(data_norm)[colnames(data_norm)=="modified_data$SalePrice"] <- "SalePrice"
numsonly <- unlist(lapply(modified_data, is.numeric))
numarray = temp[,numsonly]
numarray = subset(numarray, select = -c(Id))
fit = kmeans(numarray,4)
plotcluster(numarray,fit$cluster)
#str(fit)
fit = kmodes(numarray, 4)
plotcluster(numarray,fit$cluster)
# It is not meaningful to have clustering for categorical variables. I did only numerical values.
set.seed(1)
#Splitting training to 80%, test to 20%
index <- sample(1:nrow(data_norm), 0.80 *nrow(data_norm))
data_train <- data_norm[index,]
data_test <- data_norm[-index,]
#Our label is the Sales price, in col 1
trainlabel <- data_train[,1]
testlabel <- data_test[,1]
#Applying KNN
##test_pred <- knn(train = data_train[,2:39], test = data_test[,2:39],cl = data_train[,1], k=9)
#Creating accuracy matrix
##CrossTable(x=testlabel, y=test_pred, prop.chisq=FALSE)
# install.packages("cowplot")
# library("cowplot")
# install.packages("randomForest")
# library("randomForest")
set.seed(42)
data.imputed = randomForest(data_train$SalePrice ~ ., data = data_train, iter = 5)
data.imputed = randomForest(data_train$SalePrice ~ ., data = data_train, iter = 4)
str(data_train)
rf_data =subset(modified_data, select = -c(GarageYrBlt))
# install.packages("cowplot")
# library("cowplot")
# install.packages("randomForest")
# library("randomForest")
set.seed(42)
rdata_norm = as.data.frame(lapply(rf_data[2:39], normalize))
rdata_norm <- cbind(modified_data$SalePrice, rdata_norm)
colnames(rdata_norm)[colnames(rdata_norm)=="modified_data$SalePrice"] <- "SalePrice"
#Splitting training to 80%, test to 20%
index <- sample(1:nrow(rdata_norm), 0.80 *nrow(rdata_norm))
rdata_train <- data_norm[index,]
rdata_test <- data_norm[-index,]
data.imputed = randomForest(data_train$SalePrice ~ ., data = data_train, iter = 4)
data.imputed = randomForest(rdata_train$SalePrice ~ ., data = data_train, iter = 4)
data.imputed = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 4)
str(rdata_train)
rf_data =subset(modified_data, select = -c(GarageYrBlt))
rf_data =subset(modified_data, select = -c(GarageYrBlt))
str(rdata_data)
str(rf_data)
rdata_norm = as.data.frame(lapply(rf_data[2:39], normalize))
rdata_norm <- cbind(modified_data$SalePrice, rdata_norm)
colnames(rdata_norm)[colnames(rdata_norm)=="modified_data$SalePrice"] <- "SalePrice"
#Splitting training to 80%, test to 20%
index <- sample(1:nrow(rdata_norm), 0.80 *nrow(rdata_norm))
rdata_train <- data_norm[index,]
rdata_train <- rdata_norm[index,]
rdata_test <- rdata_norm[-index,]
data.imputed = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 4)
data.imputed
rf_data = modified_data
str(rf_data)
rf_data$GarageYrBlt = lapply(modified_data$GarageYrBlt, as.numeric)
rf_data$GarageYrBlt = lapply(modified_data$GarageYrBlt, as.numeric)
str(rf_data)
View(rf_data)
rdata_norm = as.data.frame(lapply(rf_data[2:40], normalize))
rdata_norm <- cbind(modified_data$SalePrice, rdata_norm)
colnames(rdata_norm)[colnames(rdata_norm)=="modified_data$SalePrice"] <- "SalePrice"
#Splitting training to 80%, test to 20%
index <- sample(1:nrow(rdata_norm), 0.80 *nrow(rdata_norm))
rdata_train <- rdata_norm[index,]
rdata_test <- rdata_norm[-index,]
data.imputed = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 4)
length(modified_data)
rdata_norm = as.data.frame(lapply(rf_data[2:41], normalize))
rdata_norm <- cbind(modified_data$SalePrice, rdata_norm)
colnames(rdata_norm)[colnames(rdata_norm)=="modified_data$SalePrice"] <- "SalePrice"
#Splitting training to 80%, test to 20%
index <- sample(1:nrow(rdata_norm), 0.80 *nrow(rdata_norm))
rdata_train <- rdata_norm[index,]
rdata_test <- rdata_norm[-index,]
data.imputed = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 4)
rdata_norm = as.data.frame(lapply(rf_data[2:410, normalize))
colnames(rdata_norm)[colnames(rdata_norm)=="modified_data$SalePrice"] <- "SalePrice"
data.imputed1 = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 4)
str(modified_data$GarageYrBlt)
str(rf_data$GarageYrBlt)
rf_data$GarageYrBlt = unlist(lapply(modified_data$GarageYrBlt, as.numeric), use.name = FALSE )
rdata_norm = as.data.frame(lapply(rf_data[2:40], normalize))
rdata_norm <- cbind(modified_data$SalePrice, rdata_norm)
colnames(rdata_norm)[colnames(rdata_norm)=="modified_data$SalePrice"] <- "SalePrice"
#Splitting training to 80%, test to 20%
index <- sample(1:nrow(rdata_norm), 0.80 *nrow(rdata_norm))
rdata_train <- rdata_norm[index,]
rdata_test <- rdata_norm[-index,]
data.imputed1 = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 4)
str(modified_data$GarageYrBlt)
str(rf_data$GarageYrBlt)
view(rf_data)
View(rf_data)
View(modified_data)
View(data_norm)
View(rdata_norm)
# Our Normalizing technique
normalize <- function(x) {
if (is.numeric(x) || colnames(x) == 'GarageYrBlt'){
return ((x - min(x)) / (max(x) - min(x))) }
else{
return (x)
}
}
data_norm = as.data.frame(lapply(modified_data[2:40], normalize))
# Our Normalizing technique
normalize <- function(x) {
if (is.numeric(x) || colnames(x) != 'GarageYrBlt'){
return ((x - min(x)) / (max(x) - min(x))) }
else{
return (x)
}
}
data_norm = as.data.frame(lapply(modified_data[2:40], normalize))
# Our Normalizing technique
normalize <- function(x) {
if (is.numeric(x)){
return ((x - min(x)) / (max(x) - min(x))) }
else{
return (x)
}
}
modified_data1 = subset(modified_data, select=-c(GarageYrBlt))
data_norm = as.data.frame(lapply(modified_data1[2:39], normalize))
modified_data1$GarageBltYr  = modified_data$GarageYrBlt
data_norm <- cbind(modified_data$SalePrice, data_norm)
colnames(data_norm)[colnames(data_norm)=="modified_data$SalePrice"] <- "SalePrice"
View(data_norm)
# Our Normalizing technique
normalize <- function(x) {
if (is.numeric(x)){
return ((x - min(x)) / (max(x) - min(x))) }
else{
return (x)
}
}
modified_data1 = subset(modified_data, select=-c(GarageYrBlt))
data_norm = as.data.frame(lapply(modified_data1[2:39], normalize))
data_norm$GarageBltYr  = modified_data$GarageYrBlt
data_norm <- cbind(modified_data$SalePrice, data_norm)
colnames(data_norm)[colnames(data_norm)=="modified_data$SalePrice"] <- "SalePrice"
numsonly <- unlist(lapply(modified_data, is.numeric))
numarray = temp[,numsonly]
numarray = subset(numarray, select = -c(Id))
fit = kmeans(numarray,4)
plotcluster(numarray,fit$cluster)
#str(fit)
fit = kmodes(numarray, 4)
plotcluster(numarray,fit$cluster)
# It is not meaningful to have clustering for categorical variables. I did only numerical values.
set.seed(1)
#Splitting training to 80%, test to 20%
index <- sample(1:nrow(data_norm), 0.80 *nrow(data_norm))
data_train <- data_norm[index,]
data_test <- data_norm[-index,]
# install.packages("cowplot")
# library("cowplot")
# install.packages("randomForest")
# library("randomForest")
set.seed(42)
rf_data = modified_data
rf_data$GarageYrBlt = unlist(lapply(modified_data$GarageYrBlt, as.numeric), use.name = FALSE )
rdata_norm = as.data.frame(lapply(rf_data[2:40], normalize))
rdata_norm <- cbind(modified_data$SalePrice, rdata_norm)
colnames(rdata_norm)[colnames(rdata_norm)=="modified_data$SalePrice"] <- "SalePrice"
#Splitting training to 80%, test to 20%
index <- sample(1:nrow(rdata_norm), 0.80 *nrow(rdata_norm))
rdata_train <- rdata_norm[index,]
rdata_test <- rdata_norm[-index,]
data.imputed1 = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 4)
data.imputed1 = randomForest(data_train$SalePrice ~ ., data = rdata_train, iter = 4)
data.imputed1 = randomForest(data_train$SalePrice ~ ., data = data_train, iter = 4)
#rf_data$GarageYrBlt = unlist(lapply(modified_data$GarageYrBlt, as.numeric), use.name = FALSE )
rf_data = subset(rf_data, select= -c(GarageBltYr))
rdata_norm = as.data.frame(lapply(rf_data[2:40], normalize))
#rf_data$GarageYrBlt = unlist(lapply(modified_data$GarageYrBlt, as.numeric), use.name = FALSE )
rf_data = subset(rf_data, select= -c(GarageBltYr))
#rf_data$GarageYrBlt = unlist(lapply(modified_data$GarageYrBlt, as.numeric), use.name = FALSE )
rf_data = subset(rf_data, select= -c(GarageYrBlt))
rdata_norm = as.data.frame(lapply(rf_data[2:40], normalize))
rdata_norm <- cbind(modified_data$SalePrice, rdata_norm)
colnames(rdata_norm)[colnames(rdata_norm)=="modified_data$SalePrice"] <- "SalePrice"
#Splitting training to 80%, test to 20%
index <- sample(1:nrow(rdata_norm), 0.80 *nrow(rdata_norm))
rdata_train <- rdata_norm[index,]
rdata_test <- rdata_norm[-index,]
data.imputed1 = randomForest(data_train$SalePrice ~ ., data = data_train, iter = 4)
data.imputed1 = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 4)
data.imputed1
# Our Normalizing technique
normalize <- function(x) {
if (is.numeric(x)){
return ((x - min(x)) / (max(x) - min(x))) }
else{
return (x)
}
}
data_norm = as.data.frame(lapply(modified_data[2:40], normalize))
data_norm <- cbind(modified_data$SalePrice, data_norm)
colnames(data_norm)[colnames(data_norm)=="modified_data$SalePrice"] <- "SalePrice"
data.imputed1 = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 6)
data.imputed1
data.imputed1 = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 10)
data.imputed1
data.imputed1 = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 8)
data.imputed1
data.imputed1 = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 15)
data.imputed1
data.imputed1 = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 12)
data.imputed1 = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 12)
data.imputed1
data.imputed1 = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 20)
data.imputed1
data.imputed1 = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 17)
data.imputed1
data.imputed1 = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 15)
data.imputed1
data.imputed1 = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 18)
data.imputed1
data.imputed1 = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 20)
data.imputed1
data.imputed1 = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 19)
data.imputed1
data.imputed1 = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 19)
data.imputed1
data.imputed1 = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 10)
data.imputed1
data.imputed1 = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, iter = 6)
data.imputed1
data.imputed1 = rfImpute(rdata_train$SalePrice ~ ., data = rdata_train, iter = 6)
data.imputed1 = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, proximity = TRUW)
data.imputed1 = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, proximity = TRUE)
data.imputed1
model = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, proximity = TRUE)
model
str(model)
# install.packages("ggplot2",repos = "http://cran.us.r-project.org")
# install.packages("corrplot",repos = "http://cran.us.r-project.org")
# install.packages("ggcorrplot",repos = "http://cran.us.r-project.org")
# install.packages("klaR",repos = "http://cran.us.r-project.org")
# install.packages("cluster",repos = "http://cran.us.r-project.org")
# install.packages("fpc",repos = "http://cran.us.r-project.org")
# install.packages("class",repos = "http://cran.us.r-project.org")
# install.packages("rpart",repos = "http://cran.us.r-project.org")
# install.packages("cowplot")
# install.packages("randomForest")
install.packages("rpart.plot")
library("rpart.plot")
m1 <- rpart(data_train$SalePrice ~ ., data = rdata_train, method = "anova")
m1
rpart.plot(m1, type=3, digits=3, fallen.leaves = TRUE)
rpart.plot(m1, type=2, digits=3, fallen.leaves = TRUE)
rpart.plot(m1, type=1, digits=3, fallen.leaves = TRUE)
rpart.plot(m1, type=3, digits=3, fallen.leaves = TRUE)
rpart.plot(m1, type=1, digits=3, fallen.leaves = TRUE)
m1 <- rpart(data_train$SalePrice ~ ., data = data_train, method = "anova")
m1
rpart.plot(m1, type=1, digits=3, fallen.leaves = TRUE)
m1
rpart.plot(m1, type=3, digits=3, fallen.leaves = TRUE)
rpart.plot(m1, type=2, digits=3, fallen.leaves = TRUE)
p1 <- predict(m1, data_test)
# Our Normalizing technique
normalize <- function(x) {
if (is.numeric(x)){
return ((x - min(x)) / (max(x) - min(x))) }
else{
return (x)
}
}
data_norm = as.data.frame(lapply(modified_data[2:40], normalize))
data_norm <- cbind(modified_data$SalePrice, data_norm)
colnames(data_norm)[colnames(data_norm)=="modified_data$SalePrice"] <- "SalePrice"
set.seed(1)
#Splitting training to 80%, test to 20%
index <- sample(1:nrow(data_norm), 0.80 *nrow(data_norm))
data_train <- data_norm[index,]
data_test <- data_norm[-index,]
m1 <- rpart(data_train$SalePrice ~ ., data = data_train, method = "anova")
m1
rpart.plot(m1, type=2, digits=3, fallen.leaves = TRUE)
p1 <- predict(m1, data_test)
p1
MAE <- functions(actual, predicted) {
MAE <- functions(actual, predicted){
MAE <- functions(actual, predicted){
mahmah <- functions(actual, predicted){
mahmah <- functions(actual, predicted) {mean(abs(actual- predicted))}
MAE <- function(actual, predicted){
mean(abs(actual- predicted))
}
MAE(data_test$SalePrice, p1)
MAE <- function(actual, predicted){
mean(abs(actual- predicted))
}
RMSE <- function(actual, predicted){
sqrt(mean((predicted-actual)^2))
}
MAE(data_test$SalePrice, p1)
RMSE(data_test$SalePrice, p1)
set.seed(42)
# rf_data = modified_data
# rf_data$GarageYrBlt = unlist(lapply(modified_data$GarageYrBlt, as.numeric), use.name = FALSE )
# rf_data = subset(rf_data, select= -c(GarageYrBlt))
#
# rdata_norm = as.data.frame(lapply(rf_data[2:40], normalize))
# rdata_norm <- cbind(modified_data$SalePrice, rdata_norm)
# colnames(rdata_norm)[colnames(rdata_norm)=="modified_data$SalePrice"] <- "SalePrice"
#Splitting training to 80%, test to 20%
# index <- sample(1:nrow(rdata_norm), 0.80 *nrow(rdata_norm))
# rdata_train <- rdata_norm[index,]
# rdata_test <- rdata_norm[-index,]
#model = randomForest(rdata_train$SalePrice ~ ., data = rdata_train, proximity = TRUE)
m1 <- rpart(data_train$SalePrice ~ ., data = data_train, method = "anova")
m1
rpart.plot(m1, type=2, digits=3, fallen.leaves = TRUE)
p1 <- predict(m1, data_test)
p1
MAE <- function(actual, predicted){
mean(abs(actual- predicted))
}
RMSE <- function(actual, predicted){
sqrt(mean((predicted-actual)^2))
}
MAE(data_test$SalePrice, p1)
RMSE(data_test$SalePrice, p1)
p1
m1
regress_plot <- rpart.plot(m1, type=2, digits=3, fallen.leaves = TRUE)
plot(regress_plot)
text(regress_plot, use.n = TRUE)
plot(regress_plot)
text(regress_plot, use.n = TRUE)
regress_plot <- rpart.plot(m1, type=2, digits=3, fallen.leaves = TRUE)
plot(regress_plot)
text(regress_plot, use.n = TRUE)
plotcp(regress_plot)
regress_plot <- rpart.plot(m1, type=2, digits=3, fallen.leaves = TRUE)
plot(regress_plot)
text(regress_plot, use.n = TRUE)
plotcp(regress_plot)
library((tree)
)
library(tree)
# install.packages("corrplot",repos = "http://cran.us.r-project.org")
# install.packages("ggcorrplot",repos = "http://cran.us.r-project.org")
# install.packages("klaR",repos = "http://cran.us.r-project.org")
# install.packages("cluster",repos = "http://cran.us.r-project.org")
# install.packages("fpc",repos = "http://cran.us.r-project.org")
# install.packages("class",repos = "http://cran.us.r-project.org")
# install.packages("rpart",repos = "http://cran.us.r-project.org")
# install.packages("cowplot")
# install.packages("randomForest")
# install.packages("rpart.plot")
install.packages("tree")
library("tree")
# install.packages("corrplot",repos = "http://cran.us.r-project.org")
# install.packages("ggcorrplot",repos = "http://cran.us.r-project.org")
# install.packages("klaR",repos = "http://cran.us.r-project.org")
# install.packages("cluster",repos = "http://cran.us.r-project.org")
# install.packages("fpc",repos = "http://cran.us.r-project.org")
# install.packages("class",repos = "http://cran.us.r-project.org")
# install.packages("rpart",repos = "http://cran.us.r-project.org")
# install.packages("cowplot")
# install.packages("randomForest")
# install.packages("rpart.plot")
install.packages("tree",repos = "http://cran.us.r-project.org")
